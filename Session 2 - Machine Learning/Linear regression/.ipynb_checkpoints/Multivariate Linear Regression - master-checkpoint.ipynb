{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Multiple Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing all necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "%matplotlib inline \n",
    "\n",
    "mpl.style.available\n",
    "mpl.style.use('ggplot') \n",
    "\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "import plotly.plotly as py\n",
    "py.sign_in('theasi', 'dzo58800at')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load LinRegMultiDataset.csv onto a data frame. Use pd.read_csv(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/LinRegMultiDataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore first lines and data format using df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use plt.scatter() to plot X1 against Y, then X2 against Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.scatter(df.X1,df.Y)\n",
    "plt.scatter(df.X2,df.Y, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write function to scale features according to:\n",
    "\n",
    "$$x_i = \\frac{x_i - \\mu_i}{std(x_i)}$$\n",
    "\n",
    "Use mean() and max() operators built into pandas to do this.\n",
    "The function should should return both the scaled input vector, as well as the mean and standard deviation to be used to scale future samples.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def FeatureScale(x):\n",
    "    mu = x.mean()\n",
    "    std = x.std()\n",
    "    x_scaled = (x - mu) / std\n",
    "    return x_scaled, mu, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale both input features and store in new columns in data frame (also storing mean and standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['X1_scaled'], X1_mu, X1_std = FeatureScale(df.X1)\n",
    "print df['X1'].max()\n",
    "df['X2_scaled'], X2_mu, X2_std = FeatureScale(df.X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replot using plt.scatter() to check the scaling effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.scatter(df.X1_scaled,df.Y)\n",
    "plt.scatter(df.X2_scaled,df.Y, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variate Linear Function\n",
    "(with two x inputs and three variables)\n",
    "\n",
    "$$y =  \\theta_0+\\theta_1 x_1 + \\theta_2 x_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LinFunc(x1, x2, theta_0, theta_1, theta_2):\n",
    "    y = theta_0 + theta_1*x1 + theta_2*x2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variate Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define cost function:\n",
    "$$C = \\frac{1}{2m} \\sum_{i=1}^m (y_{pred}^i - y_{obs}^i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CostFunction(y_obs,y_pred):\n",
    "    cost = np.sum((y_pred - y_obs)**2) / (2*np.size(y_pred))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define gradient function:\n",
    "$$g_j = \\frac{1}{m} \\sum_{i=1}^m (y_{pred}^i - y_{obs}^i).x^i_j$$\n",
    "Which takes both predicted and observed values, and a feature vector (x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def g(y_obs,y_pred,x):\n",
    "    g = (1/float(np.size(y_pred))) * np.sum((y_pred - y_obs)*x)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** <br>Finish gradient descent algorithm where:\n",
    "$$\\theta_j := \\theta_j - \\alpha g_j$$\n",
    "remembering that \n",
    "$$x_0^i = 1 \\quad \\text{for all } i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GradDescent(df, theta_0,theta_1,theta_2,alpha,num_iters):\n",
    "    \n",
    "    #C_history records values for plotting\n",
    "    C_history = []\n",
    "    \n",
    "    #Initalises Ypredicted to ensure existence in dataframe\n",
    "    df['Ypredicted'] = df.Y\n",
    "    \n",
    "    for i in np.arange(num_iters):\n",
    "        #Calculate prediction for parameter values\n",
    "        df.Ypredicted = LinFunc(df.X1_scaled, df.X2_scaled, theta_0, theta_1, theta_2)\n",
    "        \n",
    "        #Update parameters\n",
    "        theta_0 = theta_0 - alpha * g(df.Y,df.Ypredicted,1) \n",
    "        theta_1 = theta_1 - alpha * g(df.Y,df.Ypredicted,df.X1_scaled)\n",
    "        theta_2 = theta_2 - alpha * g(df.Y,df.Ypredicted,df.X2_scaled) \n",
    "            \n",
    "        C_history.append([CostFunction(df.Y,df.Ypredicted)])\n",
    "        \n",
    "        \n",
    "    print \"Finished at \", [theta_0,theta_1,theta_2]\n",
    "    return theta_0, theta_1, theta_2, np.array(C_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set initial guess for all three parameters and alpha value, then run Gradient Descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_0_guess = 0\n",
    "theta_1_guess = 0\n",
    "theta_2_guess = 0\n",
    "alpha = 0.1\n",
    "num_iters = 50\n",
    "\n",
    "theta_0, theta_1, theta_2, C_history = GradDescent(df, theta_0_guess,theta_1_guess,theta_2_guess,alpha,num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use plt.plot() to plot the Cost function history and ensure convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(C_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Use trained parameters to predict output when x1 = 2200 and x2 = 3.\n",
    "Remember to scale input features accordingly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = (2200 - X1_mu) / X1_std\n",
    "x2 = (3 - X2_mu) / X2_std\n",
    "\n",
    "Ypred = LinFunc(x1, x2, theta_0, theta_1, theta_2)\n",
    "print Ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back and re-run algorithm, modifying alpha and re-plotting cost function history to see how alpha influences convergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
